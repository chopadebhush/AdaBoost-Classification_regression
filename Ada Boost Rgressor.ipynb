{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e1be16c-85b5-493c-bfaa-b0975030126d",
   "metadata": {},
   "source": [
    "Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6c0757-316e-4ab8-91bc-1b0d7937093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe464667-9313-439f-97be-fb5629aa8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X ,y =make_regression(n_samples=2000,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9248fff9-9d73-496d-b0a8-45591ee64903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24656308, -0.00262717,  1.16177117, ..., -0.14381404,\n",
       "         1.04897789,  0.36642707],\n",
       "       [ 1.98789962, -0.74289827,  0.46392306, ..., -0.07953001,\n",
       "         0.88431814, -0.12744446],\n",
       "       [-1.18110317, -0.4599301 ,  1.07125243, ...,  0.41026575,\n",
       "         0.75133724,  0.39310924],\n",
       "       ...,\n",
       "       [ 0.10949255, -0.93311559,  1.06941626, ..., -0.15958241,\n",
       "        -1.09792226,  0.16945106],\n",
       "       [-0.59682953,  0.91196902,  0.32905821, ..., -0.49513463,\n",
       "         1.30232475, -0.67180649],\n",
       "       [ 0.88513231, -0.95744249,  1.24415716, ..., -0.03960346,\n",
       "        -0.44315668,  1.04897287]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce789985-0b9f-4398-a0dc-bd49cb76bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-244.6379103 ,  338.19201455, -369.33489019, ...,   89.84615892,\n",
       "         40.23247176, -113.19250449])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d97c08-f387-444b-941f-e89d177d3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train ,X_test ,y_train ,y_test =train_test_split(X,y,test_size=0.3,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fa7ad7-bccd-49ab-b244-7ab2c0afb7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3102569 , -0.35427619,  0.39650492, ...,  1.26959618,\n",
       "        -0.30150675, -2.26888313],\n",
       "       [ 1.28044403,  1.73871889,  0.84672379, ..., -0.51392111,\n",
       "        -1.44702872, -1.61237671],\n",
       "       [ 0.58455701, -0.18854658,  0.10667214, ...,  0.3039744 ,\n",
       "        -0.61025316, -1.2003673 ],\n",
       "       ...,\n",
       "       [-0.87636515,  2.1606535 ,  0.69440495, ...,  0.68105216,\n",
       "         0.4741391 , -1.15527546],\n",
       "       [ 0.8521572 , -0.50782106, -0.47473822, ..., -2.60530035,\n",
       "         1.69787155, -0.57837416],\n",
       "       [-0.92166369,  1.42898437,  0.15152283, ...,  1.97076816,\n",
       "        -2.0457776 , -0.01832535]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a94d54-c6db-4752-a578-e52e14302701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.47913336e+00, -1.32941774e+00,  7.55895123e-01, ...,\n",
       "         4.32523466e-01,  6.55392088e-01, -4.89171664e-01],\n",
       "       [ 1.21935522e+00,  9.17253872e-01, -6.24653153e-02, ...,\n",
       "         6.51603617e-01,  3.19617561e-02,  5.35192360e-01],\n",
       "       [ 1.59296974e+00,  1.29511140e-01,  3.47589377e-01, ...,\n",
       "         1.87142477e-01,  3.10548285e-02,  9.68582688e-01],\n",
       "       ...,\n",
       "       [-1.62038450e-01,  7.36388470e-01,  1.29887994e+00, ...,\n",
       "         1.34411245e+00,  9.42955805e-01,  6.33662061e-01],\n",
       "       [-1.37604633e-03,  7.85995759e-01,  8.75557342e-02, ...,\n",
       "         7.93833204e-01, -2.16655900e+00, -1.92740972e+00],\n",
       "       [ 1.92506088e+00,  1.44194971e+00,  1.00439787e+00, ...,\n",
       "         4.27048308e-01,  2.08982884e-01, -1.65906685e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414a18eb-e239-433b-bd74-3f54314bb9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.87332532e+01, -1.88437079e+02,  3.17611873e+02, -1.24581396e+02,\n",
       "        1.91836928e+02,  9.22488829e+01,  8.32273182e+01,  6.56844204e+01,\n",
       "       -5.98318024e+01,  2.58257045e+02, -8.14918748e+01, -1.30935198e+01,\n",
       "        1.15810407e+02,  7.71555155e+01, -6.92389161e+01,  7.39129851e+01,\n",
       "        1.40338454e+02, -8.97657968e+01, -7.77941160e+01, -2.08445508e+02,\n",
       "       -1.81232813e+01,  2.53718724e+02,  5.09459617e+01,  7.40800609e+01,\n",
       "        1.03898585e+02,  1.53045039e+02,  3.48614365e+01,  1.48043815e+01,\n",
       "        8.22162125e+01, -2.36006741e+02, -4.99362568e+01,  8.94147711e+01,\n",
       "        2.01256134e+01,  2.98980751e+02,  3.34942498e+01, -9.23263181e+01,\n",
       "        1.03895724e+02,  1.07130803e+02,  1.96317472e+01, -1.79255668e+02,\n",
       "        2.63988231e+02,  1.15810407e+02, -5.98318024e+01,  1.16970052e+02,\n",
       "        1.20981596e+02,  1.29502422e+01,  1.70001258e+02, -3.40671372e+01,\n",
       "        2.95820875e+01,  1.15810407e+02,  2.57526074e+02, -5.70416306e+01,\n",
       "       -1.30935198e+01, -1.24827761e+02,  2.45248548e+00,  2.62565729e+02,\n",
       "       -1.79255668e+02, -1.86463153e+02,  2.59028560e+01,  1.49104246e+01,\n",
       "       -1.87694461e+02,  3.27707183e+01,  1.45870937e+02, -1.32779845e+02,\n",
       "       -2.41483952e+02,  1.16970052e+02,  1.07130803e+02,  2.47768120e+01,\n",
       "        4.01816844e+01,  1.07130803e+02, -1.11223991e+02,  1.45870937e+02,\n",
       "        8.84937298e+01, -4.99362568e+01,  1.53045039e+02,  1.03895724e+02,\n",
       "       -3.40671372e+01,  2.24266614e+01,  1.40338454e+02,  1.15810407e+02,\n",
       "        1.18581728e+01,  8.22162125e+01, -1.24827761e+02,  1.49104246e+01,\n",
       "       -4.23666170e+01,  1.15810407e+02,  1.20263193e+02, -4.99362568e+01,\n",
       "       -1.52370343e+02,  1.70001258e+02,  1.96317472e+01,  1.73047652e+02,\n",
       "        2.97302839e+01,  3.26363580e+01,  7.40800609e+01,  5.93987504e+01,\n",
       "       -1.04975143e+02, -1.59135804e+02,  5.98177149e+00,  7.29903138e+01,\n",
       "        3.34942498e+01, -1.32779845e+02, -1.86463153e+02,  8.84937298e+01,\n",
       "        3.34942498e+01,  1.31976391e+02, -1.11869478e+02, -5.87332532e+01,\n",
       "       -4.23666170e+01, -7.77941160e+01,  3.13320549e+01, -1.24581396e+02,\n",
       "        2.77302471e+01, -1.07795082e+02,  2.95820875e+01, -6.94427016e+01,\n",
       "       -9.46609947e+01, -5.70416306e+01,  1.43084438e+02,  1.48043815e+01,\n",
       "        5.55862515e+01,  1.30473904e+02,  3.13320549e+01,  1.45464105e+02,\n",
       "       -1.20008988e+02, -5.80346023e+01, -1.51146688e+02,  3.13320549e+01,\n",
       "        2.47768120e+01,  1.54818715e+02, -9.87076164e+00,  1.32798367e+02,\n",
       "        1.45464105e+02,  2.79898468e+02,  1.30473904e+02,  2.24266614e+01,\n",
       "        1.38626541e+02,  7.39129851e+01, -6.58841738e+01,  1.18581728e+01,\n",
       "       -8.16690362e+00, -5.87332532e+01,  1.54818715e+02,  7.40800609e+01,\n",
       "       -1.04975143e+02, -6.94427016e+01, -1.98140216e+02,  1.31976391e+02,\n",
       "       -1.04975143e+02, -1.30935198e+01,  2.62565729e+02, -1.00380662e+02,\n",
       "       -2.51240612e+02, -1.79255668e+02,  1.20263193e+02,  1.38807683e-01,\n",
       "        1.30473904e+02,  5.98177149e+00,  7.71555155e+01, -5.72483274e+01,\n",
       "        1.49104246e+01, -9.53441670e+01, -5.87332532e+01, -3.40671372e+01,\n",
       "        2.79649082e+02, -5.93844953e+01, -2.00928510e+02,  1.40338454e+02,\n",
       "        1.73047652e+02,  8.22162125e+01, -9.87076164e+00,  1.45870937e+02,\n",
       "       -1.04975143e+02, -4.31352943e+01,  1.30473904e+02,  4.12863076e+01,\n",
       "       -4.99362568e+01, -5.98318024e+01, -5.70416306e+01,  1.03898585e+02,\n",
       "       -3.40671372e+01, -4.78460894e+01, -1.51146688e+02, -1.68137559e+02,\n",
       "       -1.07795082e+02,  2.45248548e+00, -8.48167940e+01,  1.18581728e+01,\n",
       "        2.98980751e+02, -8.49603663e+01, -1.42532571e+01, -1.50482965e+02,\n",
       "        1.91836928e+02, -1.07795082e+02,  1.70001258e+02,  1.32798367e+02,\n",
       "       -6.58841738e+01, -8.97657968e+01,  1.70001258e+02,  1.03666679e+00,\n",
       "        6.56844204e+01, -1.37079967e+02,  3.27707183e+01, -1.07795082e+02,\n",
       "        1.12672255e+02,  2.24266614e+01, -8.48167940e+01,  1.49104246e+01,\n",
       "        1.00689415e+02,  1.31014973e+02,  1.54818715e+02,  1.31976391e+02,\n",
       "        3.27707183e+01, -2.60667069e+01, -1.78684015e+01, -1.30935198e+01,\n",
       "       -4.31352943e+01, -1.30935198e+01,  1.18581728e+01,  1.31976391e+02,\n",
       "       -1.70893049e+02,  1.29502422e+01, -1.86463153e+02, -9.98996738e+01,\n",
       "        9.23057242e+00,  2.59028560e+01,  1.96317472e+01,  5.98177149e+00,\n",
       "       -2.45242774e+02,  5.93987504e+01, -1.68137559e+02,  5.93987504e+01,\n",
       "       -5.70416306e+01, -5.87332532e+01,  3.13320549e+01, -9.91915730e+01,\n",
       "       -1.24581396e+02,  2.59028560e+01,  1.40338454e+02, -4.99362568e+01,\n",
       "       -1.24827761e+02, -1.30935198e+01,  5.62125336e+01,  1.47038346e+02,\n",
       "       -1.81232813e+01, -7.62680298e+01,  1.91836928e+02, -4.23666170e+01,\n",
       "        8.84937298e+01, -2.56969211e+01,  1.07130803e+02,  1.45464105e+02,\n",
       "        1.43084438e+02, -1.04975143e+02, -1.30935198e+01, -1.49147801e+02,\n",
       "        1.73047652e+02,  5.55862515e+01,  1.49104246e+01,  4.64478540e+01,\n",
       "        7.40800609e+01,  1.40338454e+02, -1.11869478e+02,  1.45464105e+02,\n",
       "       -2.28977555e+02, -2.00928510e+02, -5.98318024e+01,  3.34942498e+01,\n",
       "       -2.39210807e+02,  2.01256134e+01, -9.53441670e+01,  1.47038346e+02,\n",
       "       -1.37079967e+02, -8.58403411e+01,  7.23827744e+01, -1.68137559e+02,\n",
       "       -2.00928510e+02, -1.43767119e+02, -6.92389161e+01,  2.01256134e+01,\n",
       "        7.35311785e+01,  7.39129851e+01,  2.59028560e+01,  2.24266614e+01,\n",
       "        1.32798367e+02,  4.64478540e+01, -3.40671372e+01,  1.45870937e+02,\n",
       "       -8.97657968e+01,  1.35965196e+02,  1.47544657e+02,  9.22488829e+01,\n",
       "       -1.49147801e+02, -1.97966151e+02,  1.53045039e+02,  1.03898585e+02,\n",
       "        1.15810407e+02, -1.52370343e+02,  9.22488829e+01, -9.46609947e+01,\n",
       "        5.98177149e+00, -1.77081231e+02,  2.36043333e+01,  4.27584391e+01,\n",
       "       -1.30935198e+01, -5.70416306e+01,  1.42373724e+02, -2.69436992e+02,\n",
       "        1.01676207e+02,  2.98980751e+02, -1.51146688e+02, -1.51389316e+02,\n",
       "        1.45464105e+02,  1.79027486e+02, -6.58841738e+01, -1.61497361e+02,\n",
       "       -7.77941160e+01, -5.72483274e+01,  3.13320549e+01,  1.07130803e+02,\n",
       "       -8.91199911e+01, -4.78460894e+01,  1.27572526e+02,  2.95820875e+01,\n",
       "       -5.87332532e+01, -1.51389316e+02, -8.16690362e+00,  8.84937298e+01,\n",
       "       -5.98318024e+01, -5.72483274e+01, -8.91199911e+01,  5.55862515e+01,\n",
       "        1.43084438e+02,  1.95933251e+02, -2.45731892e+02, -1.10760699e+02,\n",
       "        3.63627864e+01,  3.85252923e+01,  1.54818715e+02,  1.73047652e+02,\n",
       "       -5.80346023e+01,  2.57526074e+02,  1.38626541e+02,  1.07044234e+02,\n",
       "        2.58958060e+02,  1.15810407e+02,  1.43084438e+02, -1.81232813e+01,\n",
       "        9.23057242e+00,  1.79027486e+02,  3.92848839e+00,  1.45464105e+02,\n",
       "       -1.00380662e+02, -5.72483274e+01, -8.48167940e+01, -1.24827761e+02,\n",
       "       -5.47913004e+01,  2.47768120e+01,  4.43198258e+01, -1.07795082e+02,\n",
       "        7.23827744e+01, -1.27199989e+02, -3.14930650e+01,  1.96317472e+01,\n",
       "        7.39129851e+01,  2.59028560e+01, -5.47913004e+01,  9.23057242e+00,\n",
       "        1.32798367e+02,  2.59028560e+01, -1.79255668e+02,  1.29502422e+01,\n",
       "        1.07044234e+02, -6.58841738e+01, -2.28977555e+02, -4.31352943e+01,\n",
       "        1.06094670e+02,  1.45464105e+02, -6.11290180e+01,  2.53718724e+02,\n",
       "        1.20981596e+02,  2.53718724e+02,  1.49104246e+01, -1.79255668e+02,\n",
       "        7.39129851e+01, -3.40671372e+01,  4.74115041e+01, -4.78460894e+01,\n",
       "       -5.80346023e+01, -1.24581396e+02, -9.46609947e+01, -9.87076164e+00,\n",
       "        3.17611873e+02,  4.12863076e+01, -1.51389316e+02, -1.50482965e+02,\n",
       "        1.70001258e+02, -8.91199911e+01,  5.98177149e+00,  2.93517282e+02,\n",
       "        1.49104246e+01,  2.59028560e+01, -1.32779845e+02, -2.74165048e+02,\n",
       "        1.30473904e+02,  1.32798367e+02,  1.29502422e+01,  2.53718724e+02,\n",
       "        5.93987504e+01, -8.48167940e+01,  1.45464105e+02,  5.98177149e+00,\n",
       "       -1.37079967e+02, -6.58841738e+01, -4.99362568e+01,  8.22162125e+01,\n",
       "       -5.93844953e+01,  1.49104246e+01,  5.93987504e+01, -1.98140216e+02,\n",
       "       -1.20008988e+02, -2.36006741e+02,  3.13320549e+01,  4.12863076e+01,\n",
       "       -4.54485296e+01,  1.43084438e+02,  1.06094670e+02,  7.40800609e+01,\n",
       "        3.34942498e+01, -7.77941160e+01, -1.30935198e+01,  2.59028560e+01,\n",
       "        1.47038346e+02, -4.99362568e+01,  1.20263193e+02, -1.11223991e+02,\n",
       "        1.79027486e+02, -1.68428957e+02,  2.01256134e+01,  1.40338454e+02,\n",
       "        2.79649082e+02,  8.94147711e+01, -5.87332532e+01, -2.39210807e+02,\n",
       "        1.38626541e+02,  3.13320549e+01,  2.59028560e+01, -1.68137559e+02,\n",
       "        4.64478540e+01, -1.11869478e+02, -7.77941160e+01, -6.11290180e+01,\n",
       "       -5.72483274e+01, -1.04975143e+02,  1.96317472e+01, -1.37079967e+02,\n",
       "       -1.30935198e+01,  1.95933251e+02, -6.94427016e+01, -2.30457307e+02,\n",
       "        4.27584391e+01,  1.03898585e+02, -1.49147801e+02, -1.87694461e+02,\n",
       "        2.24266614e+01, -6.94427016e+01,  1.31976391e+02, -8.97657968e+01,\n",
       "        7.40800609e+01,  1.07044234e+02,  2.57342856e+02,  2.24266614e+01,\n",
       "       -5.70416306e+01,  6.56844204e+01,  8.22162125e+01,  1.32513002e+02,\n",
       "       -1.32779845e+02, -6.58841738e+01, -5.72483274e+01,  1.20263193e+02,\n",
       "       -5.70416306e+01, -1.37079967e+02,  1.49104246e+01, -1.42532571e+01,\n",
       "        5.93987504e+01,  1.38626541e+02, -1.97966151e+02, -5.47913004e+01,\n",
       "       -5.72483274e+01, -1.50482965e+02,  1.32798367e+02,  3.92848839e+00,\n",
       "       -1.42532571e+01, -8.91199911e+01, -6.92389161e+01, -1.10760699e+02,\n",
       "       -6.58841738e+01, -7.77941160e+01,  1.31976391e+02,  7.39129851e+01,\n",
       "       -7.77941160e+01, -1.28244236e+02, -9.87076164e+00, -1.07795082e+02,\n",
       "       -5.98318024e+01,  2.45248548e+00, -1.24827761e+02, -2.39717262e+02,\n",
       "       -8.97657968e+01, -1.51146688e+02, -1.61497361e+02,  1.47544657e+02,\n",
       "       -1.11869478e+02,  8.22162125e+01, -5.72483274e+01, -7.77941160e+01,\n",
       "        3.85252923e+01,  2.95820875e+01,  2.47768120e+01,  3.63627864e+01,\n",
       "       -5.70416306e+01, -6.58841738e+01,  3.48614365e+01,  1.45464105e+02,\n",
       "       -1.50482965e+02, -5.72483274e+01,  2.95820875e+01,  2.24266614e+01,\n",
       "        1.03898585e+02, -5.70416306e+01,  1.40338454e+02,  1.96317472e+01,\n",
       "        2.95820875e+01, -1.68428957e+02,  6.56844204e+01, -4.99362568e+01,\n",
       "        2.79649082e+02,  1.49104246e+01,  1.43084438e+02,  1.38807683e-01,\n",
       "        8.94147711e+01,  1.73047652e+02,  1.25572965e+02,  7.40800609e+01,\n",
       "        1.63081161e-01, -2.08445508e+02, -4.99362568e+01, -5.72483274e+01,\n",
       "       -1.78684015e+01, -1.10760699e+02,  8.22162125e+01,  2.53718724e+02,\n",
       "       -5.98318024e+01,  8.22162125e+01, -1.37079967e+02,  2.47768120e+01,\n",
       "        1.20263193e+02, -4.99362568e+01, -5.47913004e+01, -2.36006741e+02,\n",
       "        3.13320549e+01,  2.64355442e+02, -9.23263181e+01,  1.21879023e+02,\n",
       "       -5.93844953e+01,  8.32273182e+01, -1.50618412e+02, -5.87332532e+01,\n",
       "        9.23057242e+00, -4.99362568e+01,  2.95820875e+01, -1.30663288e+02,\n",
       "        1.97958744e+02, -5.72483274e+01, -1.87694461e+02, -5.98318024e+01,\n",
       "       -8.16690362e+00,  2.79898468e+02,  1.47038346e+02,  2.01256134e+01,\n",
       "       -5.70416306e+01, -1.30935198e+01, -9.46609947e+01, -1.52370343e+02,\n",
       "        2.36130429e+02,  3.26363580e+01, -6.58841738e+01, -2.39210807e+02,\n",
       "        1.15810407e+02,  8.94147711e+01,  5.98177149e+00, -3.40671372e+01,\n",
       "        2.79404179e+02, -2.56969211e+01,  3.34942498e+01,  2.63988231e+02,\n",
       "        3.85252923e+01, -3.14930650e+01,  1.53045039e+02, -2.39210807e+02])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "classifier =AdaBoostRegressor()\n",
    "classifier.fit(X_train ,y_train)\n",
    "y_pred =classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75cb1d4-eec7-4963-9e95-ced1165e1a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7526831001929267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_test ,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c8d8c-2f5c-45bf-b126-77f5c6d47a33",
   "metadata": {},
   "source": [
    "hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b4b8f6-7ac9-4e4d-8b24-505c53f2c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid ={\"n_estimators\":[100,150,200,250,300],\n",
    "            \"learning_rate\":[0.001,0.005,0.1,1,1.5,2],\n",
    "            \"loss\":[\"linear\",'square','exponentia']}\n",
    "ada =AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af4edce-af6a-42c8-b418-a644760393da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =GridSearchCV(estimator=ada ,param_grid=param_grid ,cv =5,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85287cd6-9519-47f6-b0d7-434b44289d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "[CV 1/5] END learning_rate=0.001, loss=linear, n_estimators=100;, score=0.509 total time=   4.1s\n",
      "[CV 2/5] END learning_rate=0.001, loss=linear, n_estimators=100;, score=0.531 total time=   4.1s\n",
      "[CV 3/5] END learning_rate=0.001, loss=linear, n_estimators=100;, score=0.544 total time=   4.1s\n",
      "[CV 4/5] END learning_rate=0.001, loss=linear, n_estimators=100;, score=0.415 total time=   4.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=linear, n_estimators=100;, score=0.517 total time=   4.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=linear, n_estimators=150;, score=0.507 total time=   6.1s\n",
      "[CV 2/5] END learning_rate=0.001, loss=linear, n_estimators=150;, score=0.542 total time=   6.2s\n",
      "[CV 3/5] END learning_rate=0.001, loss=linear, n_estimators=150;, score=0.545 total time=   6.1s\n",
      "[CV 4/5] END learning_rate=0.001, loss=linear, n_estimators=150;, score=0.418 total time=   6.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=linear, n_estimators=150;, score=0.535 total time=   6.2s\n",
      "[CV 1/5] END learning_rate=0.001, loss=linear, n_estimators=200;, score=0.510 total time=   8.2s\n",
      "[CV 2/5] END learning_rate=0.001, loss=linear, n_estimators=200;, score=0.541 total time=   8.1s\n",
      "[CV 3/5] END learning_rate=0.001, loss=linear, n_estimators=200;, score=0.547 total time=   8.2s\n",
      "[CV 4/5] END learning_rate=0.001, loss=linear, n_estimators=200;, score=0.421 total time=   8.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=linear, n_estimators=200;, score=0.539 total time=   8.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=linear, n_estimators=250;, score=0.516 total time=  10.3s\n",
      "[CV 2/5] END learning_rate=0.001, loss=linear, n_estimators=250;, score=0.544 total time=  10.2s\n",
      "[CV 3/5] END learning_rate=0.001, loss=linear, n_estimators=250;, score=0.553 total time=  10.2s\n",
      "[CV 4/5] END learning_rate=0.001, loss=linear, n_estimators=250;, score=0.413 total time=  10.2s\n",
      "[CV 5/5] END learning_rate=0.001, loss=linear, n_estimators=250;, score=0.531 total time=  10.3s\n",
      "[CV 1/5] END learning_rate=0.001, loss=linear, n_estimators=300;, score=0.512 total time=  12.2s\n",
      "[CV 2/5] END learning_rate=0.001, loss=linear, n_estimators=300;, score=0.543 total time=  12.3s\n",
      "[CV 3/5] END learning_rate=0.001, loss=linear, n_estimators=300;, score=0.555 total time=  12.2s\n",
      "[CV 4/5] END learning_rate=0.001, loss=linear, n_estimators=300;, score=0.421 total time=  12.3s\n",
      "[CV 5/5] END learning_rate=0.001, loss=linear, n_estimators=300;, score=0.534 total time=  12.3s\n",
      "[CV 1/5] END learning_rate=0.001, loss=square, n_estimators=100;, score=0.510 total time=   4.1s\n",
      "[CV 2/5] END learning_rate=0.001, loss=square, n_estimators=100;, score=0.536 total time=   4.1s\n",
      "[CV 3/5] END learning_rate=0.001, loss=square, n_estimators=100;, score=0.549 total time=   4.1s\n",
      "[CV 4/5] END learning_rate=0.001, loss=square, n_estimators=100;, score=0.414 total time=   4.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=square, n_estimators=100;, score=0.530 total time=   4.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=square, n_estimators=150;, score=0.516 total time=   6.1s\n",
      "[CV 2/5] END learning_rate=0.001, loss=square, n_estimators=150;, score=0.543 total time=   6.1s\n",
      "[CV 3/5] END learning_rate=0.001, loss=square, n_estimators=150;, score=0.546 total time=   6.1s\n",
      "[CV 4/5] END learning_rate=0.001, loss=square, n_estimators=150;, score=0.414 total time=   6.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=square, n_estimators=150;, score=0.523 total time=   6.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=square, n_estimators=200;, score=0.511 total time=   8.2s\n",
      "[CV 2/5] END learning_rate=0.001, loss=square, n_estimators=200;, score=0.542 total time=   8.2s\n",
      "[CV 3/5] END learning_rate=0.001, loss=square, n_estimators=200;, score=0.553 total time=   8.2s\n",
      "[CV 4/5] END learning_rate=0.001, loss=square, n_estimators=200;, score=0.424 total time=   8.2s\n",
      "[CV 5/5] END learning_rate=0.001, loss=square, n_estimators=200;, score=0.529 total time=   8.2s\n",
      "[CV 1/5] END learning_rate=0.001, loss=square, n_estimators=250;, score=0.513 total time=  10.2s\n",
      "[CV 2/5] END learning_rate=0.001, loss=square, n_estimators=250;, score=0.546 total time=  10.2s\n",
      "[CV 3/5] END learning_rate=0.001, loss=square, n_estimators=250;, score=0.550 total time=  10.2s\n",
      "[CV 4/5] END learning_rate=0.001, loss=square, n_estimators=250;, score=0.422 total time=  10.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=square, n_estimators=250;, score=0.529 total time=  10.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=square, n_estimators=300;, score=0.517 total time=  12.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=square, n_estimators=300;, score=0.543 total time=  12.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=square, n_estimators=300;, score=0.557 total time=  12.1s\n",
      "[CV 4/5] END learning_rate=0.001, loss=square, n_estimators=300;, score=0.426 total time=  12.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=square, n_estimators=300;, score=0.535 total time=  12.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=linear, n_estimators=100;, score=0.507 total time=   4.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=linear, n_estimators=100;, score=0.541 total time=   4.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=linear, n_estimators=100;, score=0.554 total time=   4.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=linear, n_estimators=100;, score=0.423 total time=   4.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=linear, n_estimators=100;, score=0.537 total time=   4.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=linear, n_estimators=150;, score=0.517 total time=   6.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=linear, n_estimators=150;, score=0.549 total time=   6.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=linear, n_estimators=150;, score=0.564 total time=   6.1s\n",
      "[CV 4/5] END learning_rate=0.005, loss=linear, n_estimators=150;, score=0.434 total time=   6.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=linear, n_estimators=150;, score=0.549 total time=   6.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=linear, n_estimators=200;, score=0.531 total time=   8.1s\n",
      "[CV 2/5] END learning_rate=0.005, loss=linear, n_estimators=200;, score=0.558 total time=   8.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=linear, n_estimators=200;, score=0.562 total time=   8.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=linear, n_estimators=200;, score=0.433 total time=   8.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=linear, n_estimators=200;, score=0.544 total time=   8.1s\n",
      "[CV 1/5] END learning_rate=0.005, loss=linear, n_estimators=250;, score=0.534 total time=  10.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=linear, n_estimators=250;, score=0.552 total time=  10.1s\n",
      "[CV 3/5] END learning_rate=0.005, loss=linear, n_estimators=250;, score=0.567 total time=  10.1s\n",
      "[CV 4/5] END learning_rate=0.005, loss=linear, n_estimators=250;, score=0.444 total time=  10.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=linear, n_estimators=250;, score=0.549 total time=  10.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=linear, n_estimators=300;, score=0.530 total time=  12.2s\n",
      "[CV 2/5] END learning_rate=0.005, loss=linear, n_estimators=300;, score=0.559 total time=  12.1s\n",
      "[CV 3/5] END learning_rate=0.005, loss=linear, n_estimators=300;, score=0.574 total time=  12.1s\n",
      "[CV 4/5] END learning_rate=0.005, loss=linear, n_estimators=300;, score=0.447 total time=  12.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=linear, n_estimators=300;, score=0.552 total time=  12.1s\n",
      "[CV 1/5] END learning_rate=0.005, loss=square, n_estimators=100;, score=0.513 total time=   4.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=square, n_estimators=100;, score=0.545 total time=   4.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=square, n_estimators=100;, score=0.565 total time=   4.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=square, n_estimators=100;, score=0.423 total time=   4.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=square, n_estimators=100;, score=0.551 total time=   4.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=square, n_estimators=150;, score=0.531 total time=   6.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=square, n_estimators=150;, score=0.550 total time=   6.1s\n",
      "[CV 3/5] END learning_rate=0.005, loss=square, n_estimators=150;, score=0.559 total time=   6.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=square, n_estimators=150;, score=0.440 total time=   6.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=square, n_estimators=150;, score=0.542 total time=   6.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=square, n_estimators=200;, score=0.534 total time=   8.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=square, n_estimators=200;, score=0.554 total time=   8.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=square, n_estimators=200;, score=0.571 total time=   8.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=square, n_estimators=200;, score=0.443 total time=   8.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=square, n_estimators=200;, score=0.553 total time=   8.1s\n",
      "[CV 1/5] END learning_rate=0.005, loss=square, n_estimators=250;, score=0.545 total time=  10.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=square, n_estimators=250;, score=0.563 total time=  10.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=square, n_estimators=250;, score=0.573 total time=  10.1s\n",
      "[CV 4/5] END learning_rate=0.005, loss=square, n_estimators=250;, score=0.442 total time=  10.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=square, n_estimators=250;, score=0.557 total time=  10.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=square, n_estimators=300;, score=0.546 total time=  12.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=square, n_estimators=300;, score=0.565 total time=  12.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=square, n_estimators=300;, score=0.577 total time=  12.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=square, n_estimators=300;, score=0.455 total time=  12.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=square, n_estimators=300;, score=0.556 total time=  12.1s\n",
      "[CV 1/5] END learning_rate=0.005, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.005, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.005, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.005, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.005, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.005, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=linear, n_estimators=100;, score=0.640 total time=   3.8s\n",
      "[CV 2/5] END learning_rate=0.1, loss=linear, n_estimators=100;, score=0.630 total time=   3.9s\n",
      "[CV 3/5] END learning_rate=0.1, loss=linear, n_estimators=100;, score=0.640 total time=   3.9s\n",
      "[CV 4/5] END learning_rate=0.1, loss=linear, n_estimators=100;, score=0.549 total time=   3.8s\n",
      "[CV 5/5] END learning_rate=0.1, loss=linear, n_estimators=100;, score=0.654 total time=   3.8s\n",
      "[CV 1/5] END learning_rate=0.1, loss=linear, n_estimators=150;, score=0.676 total time=   5.5s\n",
      "[CV 2/5] END learning_rate=0.1, loss=linear, n_estimators=150;, score=0.668 total time=   5.6s\n",
      "[CV 3/5] END learning_rate=0.1, loss=linear, n_estimators=150;, score=0.672 total time=   5.6s\n",
      "[CV 4/5] END learning_rate=0.1, loss=linear, n_estimators=150;, score=0.595 total time=   5.5s\n",
      "[CV 5/5] END learning_rate=0.1, loss=linear, n_estimators=150;, score=0.692 total time=   5.5s\n",
      "[CV 1/5] END learning_rate=0.1, loss=linear, n_estimators=200;, score=0.700 total time=   7.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=linear, n_estimators=200;, score=0.690 total time=   7.2s\n",
      "[CV 3/5] END learning_rate=0.1, loss=linear, n_estimators=200;, score=0.695 total time=   7.2s\n",
      "[CV 4/5] END learning_rate=0.1, loss=linear, n_estimators=200;, score=0.629 total time=   7.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=linear, n_estimators=200;, score=0.700 total time=   7.2s\n",
      "[CV 1/5] END learning_rate=0.1, loss=linear, n_estimators=250;, score=0.717 total time=   8.9s\n",
      "[CV 2/5] END learning_rate=0.1, loss=linear, n_estimators=250;, score=0.705 total time=   8.8s\n",
      "[CV 3/5] END learning_rate=0.1, loss=linear, n_estimators=250;, score=0.716 total time=   8.8s\n",
      "[CV 4/5] END learning_rate=0.1, loss=linear, n_estimators=250;, score=0.679 total time=   8.7s\n",
      "[CV 5/5] END learning_rate=0.1, loss=linear, n_estimators=250;, score=0.718 total time=   8.7s\n",
      "[CV 1/5] END learning_rate=0.1, loss=linear, n_estimators=300;, score=0.736 total time=  10.3s\n",
      "[CV 2/5] END learning_rate=0.1, loss=linear, n_estimators=300;, score=0.716 total time=  10.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=linear, n_estimators=300;, score=0.729 total time=  10.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=linear, n_estimators=300;, score=0.683 total time=  10.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=linear, n_estimators=300;, score=0.739 total time=  10.3s\n",
      "[CV 1/5] END learning_rate=0.1, loss=square, n_estimators=100;, score=0.676 total time=   3.7s\n",
      "[CV 2/5] END learning_rate=0.1, loss=square, n_estimators=100;, score=0.670 total time=   3.7s\n",
      "[CV 3/5] END learning_rate=0.1, loss=square, n_estimators=100;, score=0.678 total time=   3.7s\n",
      "[CV 4/5] END learning_rate=0.1, loss=square, n_estimators=100;, score=0.602 total time=   3.7s\n",
      "[CV 5/5] END learning_rate=0.1, loss=square, n_estimators=100;, score=0.677 total time=   3.6s\n",
      "[CV 1/5] END learning_rate=0.1, loss=square, n_estimators=150;, score=0.710 total time=   5.3s\n",
      "[CV 2/5] END learning_rate=0.1, loss=square, n_estimators=150;, score=0.706 total time=   5.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=square, n_estimators=150;, score=0.709 total time=   5.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=square, n_estimators=150;, score=0.667 total time=   5.3s\n",
      "[CV 5/5] END learning_rate=0.1, loss=square, n_estimators=150;, score=0.717 total time=   5.3s\n",
      "[CV 1/5] END learning_rate=0.1, loss=square, n_estimators=200;, score=0.733 total time=   6.9s\n",
      "[CV 2/5] END learning_rate=0.1, loss=square, n_estimators=200;, score=0.718 total time=   6.9s\n",
      "[CV 3/5] END learning_rate=0.1, loss=square, n_estimators=200;, score=0.734 total time=   6.8s\n",
      "[CV 4/5] END learning_rate=0.1, loss=square, n_estimators=200;, score=0.703 total time=   6.7s\n",
      "[CV 5/5] END learning_rate=0.1, loss=square, n_estimators=200;, score=0.759 total time=   6.7s\n",
      "[CV 1/5] END learning_rate=0.1, loss=square, n_estimators=250;, score=0.750 total time=   8.3s\n",
      "[CV 2/5] END learning_rate=0.1, loss=square, n_estimators=250;, score=0.737 total time=   8.4s\n",
      "[CV 3/5] END learning_rate=0.1, loss=square, n_estimators=250;, score=0.746 total time=   8.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=square, n_estimators=250;, score=0.722 total time=   8.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=square, n_estimators=250;, score=0.768 total time=   8.2s\n",
      "[CV 1/5] END learning_rate=0.1, loss=square, n_estimators=300;, score=0.751 total time=   9.8s\n",
      "[CV 2/5] END learning_rate=0.1, loss=square, n_estimators=300;, score=0.745 total time=   9.8s\n",
      "[CV 3/5] END learning_rate=0.1, loss=square, n_estimators=300;, score=0.760 total time=   9.7s\n",
      "[CV 4/5] END learning_rate=0.1, loss=square, n_estimators=300;, score=0.741 total time=   9.6s\n",
      "[CV 5/5] END learning_rate=0.1, loss=square, n_estimators=300;, score=0.781 total time=   9.6s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=linear, n_estimators=100;, score=0.776 total time=   2.9s\n",
      "[CV 2/5] END learning_rate=1, loss=linear, n_estimators=100;, score=0.755 total time=   2.9s\n",
      "[CV 3/5] END learning_rate=1, loss=linear, n_estimators=100;, score=0.766 total time=   2.8s\n",
      "[CV 4/5] END learning_rate=1, loss=linear, n_estimators=100;, score=0.764 total time=   2.8s\n",
      "[CV 5/5] END learning_rate=1, loss=linear, n_estimators=100;, score=0.790 total time=   2.9s\n",
      "[CV 1/5] END learning_rate=1, loss=linear, n_estimators=150;, score=0.788 total time=   4.1s\n",
      "[CV 2/5] END learning_rate=1, loss=linear, n_estimators=150;, score=0.764 total time=   4.1s\n",
      "[CV 3/5] END learning_rate=1, loss=linear, n_estimators=150;, score=0.794 total time=   4.1s\n",
      "[CV 4/5] END learning_rate=1, loss=linear, n_estimators=150;, score=0.774 total time=   4.1s\n",
      "[CV 5/5] END learning_rate=1, loss=linear, n_estimators=150;, score=0.796 total time=   4.1s\n",
      "[CV 1/5] END learning_rate=1, loss=linear, n_estimators=200;, score=0.796 total time=   5.4s\n",
      "[CV 2/5] END learning_rate=1, loss=linear, n_estimators=200;, score=0.768 total time=   5.3s\n",
      "[CV 3/5] END learning_rate=1, loss=linear, n_estimators=200;, score=0.795 total time=   5.2s\n",
      "[CV 4/5] END learning_rate=1, loss=linear, n_estimators=200;, score=0.780 total time=   5.3s\n",
      "[CV 5/5] END learning_rate=1, loss=linear, n_estimators=200;, score=0.802 total time=   5.2s\n",
      "[CV 1/5] END learning_rate=1, loss=linear, n_estimators=250;, score=0.797 total time=   6.5s\n",
      "[CV 2/5] END learning_rate=1, loss=linear, n_estimators=250;, score=0.764 total time=   6.5s\n",
      "[CV 3/5] END learning_rate=1, loss=linear, n_estimators=250;, score=0.788 total time=   6.4s\n",
      "[CV 4/5] END learning_rate=1, loss=linear, n_estimators=250;, score=0.789 total time=   6.4s\n",
      "[CV 5/5] END learning_rate=1, loss=linear, n_estimators=250;, score=0.811 total time=   6.3s\n",
      "[CV 1/5] END learning_rate=1, loss=linear, n_estimators=300;, score=0.800 total time=   7.6s\n",
      "[CV 2/5] END learning_rate=1, loss=linear, n_estimators=300;, score=0.769 total time=   7.6s\n",
      "[CV 3/5] END learning_rate=1, loss=linear, n_estimators=300;, score=0.792 total time=   7.5s\n",
      "[CV 4/5] END learning_rate=1, loss=linear, n_estimators=300;, score=0.783 total time=   7.6s\n",
      "[CV 5/5] END learning_rate=1, loss=linear, n_estimators=300;, score=0.810 total time=   7.6s\n",
      "[CV 1/5] END learning_rate=1, loss=square, n_estimators=100;, score=0.827 total time=   2.6s\n",
      "[CV 2/5] END learning_rate=1, loss=square, n_estimators=100;, score=0.797 total time=   2.6s\n",
      "[CV 3/5] END learning_rate=1, loss=square, n_estimators=100;, score=0.810 total time=   2.6s\n",
      "[CV 4/5] END learning_rate=1, loss=square, n_estimators=100;, score=0.794 total time=   2.5s\n",
      "[CV 5/5] END learning_rate=1, loss=square, n_estimators=100;, score=0.825 total time=   2.6s\n",
      "[CV 1/5] END learning_rate=1, loss=square, n_estimators=150;, score=0.828 total time=   3.7s\n",
      "[CV 2/5] END learning_rate=1, loss=square, n_estimators=150;, score=0.805 total time=   3.7s\n",
      "[CV 3/5] END learning_rate=1, loss=square, n_estimators=150;, score=0.825 total time=   3.7s\n",
      "[CV 4/5] END learning_rate=1, loss=square, n_estimators=150;, score=0.815 total time=   3.6s\n",
      "[CV 5/5] END learning_rate=1, loss=square, n_estimators=150;, score=0.834 total time=   3.6s\n",
      "[CV 1/5] END learning_rate=1, loss=square, n_estimators=200;, score=0.819 total time=   4.8s\n",
      "[CV 2/5] END learning_rate=1, loss=square, n_estimators=200;, score=0.803 total time=   4.8s\n",
      "[CV 3/5] END learning_rate=1, loss=square, n_estimators=200;, score=0.831 total time=   4.7s\n",
      "[CV 4/5] END learning_rate=1, loss=square, n_estimators=200;, score=0.821 total time=   4.7s\n",
      "[CV 5/5] END learning_rate=1, loss=square, n_estimators=200;, score=0.846 total time=   4.7s\n",
      "[CV 1/5] END learning_rate=1, loss=square, n_estimators=250;, score=0.824 total time=   5.8s\n",
      "[CV 2/5] END learning_rate=1, loss=square, n_estimators=250;, score=0.809 total time=   5.8s\n",
      "[CV 3/5] END learning_rate=1, loss=square, n_estimators=250;, score=0.836 total time=   5.7s\n",
      "[CV 4/5] END learning_rate=1, loss=square, n_estimators=250;, score=0.822 total time=   5.9s\n",
      "[CV 5/5] END learning_rate=1, loss=square, n_estimators=250;, score=0.847 total time=   5.8s\n",
      "[CV 1/5] END learning_rate=1, loss=square, n_estimators=300;, score=0.835 total time=   6.8s\n",
      "[CV 2/5] END learning_rate=1, loss=square, n_estimators=300;, score=0.806 total time=   7.0s\n",
      "[CV 3/5] END learning_rate=1, loss=square, n_estimators=300;, score=0.834 total time=   6.8s\n",
      "[CV 4/5] END learning_rate=1, loss=square, n_estimators=300;, score=0.827 total time=   6.8s\n",
      "[CV 5/5] END learning_rate=1, loss=square, n_estimators=300;, score=0.849 total time=   6.9s\n",
      "[CV 1/5] END learning_rate=1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=linear, n_estimators=100;, score=0.803 total time=   2.6s\n",
      "[CV 2/5] END learning_rate=1.5, loss=linear, n_estimators=100;, score=0.765 total time=   2.6s\n",
      "[CV 3/5] END learning_rate=1.5, loss=linear, n_estimators=100;, score=0.779 total time=   2.6s\n",
      "[CV 4/5] END learning_rate=1.5, loss=linear, n_estimators=100;, score=0.771 total time=   2.6s\n",
      "[CV 5/5] END learning_rate=1.5, loss=linear, n_estimators=100;, score=0.794 total time=   2.6s\n",
      "[CV 1/5] END learning_rate=1.5, loss=linear, n_estimators=150;, score=0.794 total time=   3.8s\n",
      "[CV 2/5] END learning_rate=1.5, loss=linear, n_estimators=150;, score=0.760 total time=   3.8s\n",
      "[CV 3/5] END learning_rate=1.5, loss=linear, n_estimators=150;, score=0.794 total time=   3.7s\n",
      "[CV 4/5] END learning_rate=1.5, loss=linear, n_estimators=150;, score=0.766 total time=   3.8s\n",
      "[CV 5/5] END learning_rate=1.5, loss=linear, n_estimators=150;, score=0.808 total time=   3.8s\n",
      "[CV 1/5] END learning_rate=1.5, loss=linear, n_estimators=200;, score=0.791 total time=   4.9s\n",
      "[CV 2/5] END learning_rate=1.5, loss=linear, n_estimators=200;, score=0.772 total time=   4.9s\n",
      "[CV 3/5] END learning_rate=1.5, loss=linear, n_estimators=200;, score=0.792 total time=   4.8s\n",
      "[CV 4/5] END learning_rate=1.5, loss=linear, n_estimators=200;, score=0.789 total time=   4.9s\n",
      "[CV 5/5] END learning_rate=1.5, loss=linear, n_estimators=200;, score=0.807 total time=   4.9s\n",
      "[CV 1/5] END learning_rate=1.5, loss=linear, n_estimators=250;, score=0.804 total time=   6.1s\n",
      "[CV 2/5] END learning_rate=1.5, loss=linear, n_estimators=250;, score=0.778 total time=   6.1s\n",
      "[CV 3/5] END learning_rate=1.5, loss=linear, n_estimators=250;, score=0.797 total time=   6.0s\n",
      "[CV 4/5] END learning_rate=1.5, loss=linear, n_estimators=250;, score=0.779 total time=   5.9s\n",
      "[CV 5/5] END learning_rate=1.5, loss=linear, n_estimators=250;, score=0.819 total time=   6.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=linear, n_estimators=300;, score=0.804 total time=   7.2s\n",
      "[CV 2/5] END learning_rate=1.5, loss=linear, n_estimators=300;, score=0.772 total time=   7.1s\n",
      "[CV 3/5] END learning_rate=1.5, loss=linear, n_estimators=300;, score=0.796 total time=   7.1s\n",
      "[CV 4/5] END learning_rate=1.5, loss=linear, n_estimators=300;, score=0.794 total time=   7.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=linear, n_estimators=300;, score=0.816 total time=   7.1s\n",
      "[CV 1/5] END learning_rate=1.5, loss=square, n_estimators=100;, score=0.825 total time=   2.1s\n",
      "[CV 2/5] END learning_rate=1.5, loss=square, n_estimators=100;, score=0.807 total time=   2.1s\n",
      "[CV 3/5] END learning_rate=1.5, loss=square, n_estimators=100;, score=0.832 total time=   2.1s\n",
      "[CV 4/5] END learning_rate=1.5, loss=square, n_estimators=100;, score=0.814 total time=   2.1s\n",
      "[CV 5/5] END learning_rate=1.5, loss=square, n_estimators=100;, score=0.828 total time=   2.1s\n",
      "[CV 1/5] END learning_rate=1.5, loss=square, n_estimators=150;, score=0.843 total time=   3.1s\n",
      "[CV 2/5] END learning_rate=1.5, loss=square, n_estimators=150;, score=0.805 total time=   3.1s\n",
      "[CV 3/5] END learning_rate=1.5, loss=square, n_estimators=150;, score=0.849 total time=   3.1s\n",
      "[CV 4/5] END learning_rate=1.5, loss=square, n_estimators=150;, score=0.828 total time=   3.1s\n",
      "[CV 5/5] END learning_rate=1.5, loss=square, n_estimators=150;, score=0.839 total time=   3.1s\n",
      "[CV 1/5] END learning_rate=1.5, loss=square, n_estimators=200;, score=0.836 total time=   4.0s\n",
      "[CV 2/5] END learning_rate=1.5, loss=square, n_estimators=200;, score=0.827 total time=   4.1s\n",
      "[CV 3/5] END learning_rate=1.5, loss=square, n_estimators=200;, score=0.854 total time=   4.0s\n",
      "[CV 4/5] END learning_rate=1.5, loss=square, n_estimators=200;, score=0.843 total time=   4.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=square, n_estimators=200;, score=0.858 total time=   4.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=square, n_estimators=250;, score=0.846 total time=   4.9s\n",
      "[CV 2/5] END learning_rate=1.5, loss=square, n_estimators=250;, score=0.823 total time=   4.9s\n",
      "[CV 3/5] END learning_rate=1.5, loss=square, n_estimators=250;, score=0.851 total time=   4.9s\n",
      "[CV 4/5] END learning_rate=1.5, loss=square, n_estimators=250;, score=0.841 total time=   5.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=square, n_estimators=250;, score=0.858 total time=   5.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=square, n_estimators=300;, score=0.854 total time=   5.9s\n",
      "[CV 2/5] END learning_rate=1.5, loss=square, n_estimators=300;, score=0.823 total time=   6.0s\n",
      "[CV 3/5] END learning_rate=1.5, loss=square, n_estimators=300;, score=0.852 total time=   5.9s\n",
      "[CV 4/5] END learning_rate=1.5, loss=square, n_estimators=300;, score=0.844 total time=   5.8s\n",
      "[CV 5/5] END learning_rate=1.5, loss=square, n_estimators=300;, score=0.868 total time=   5.8s\n",
      "[CV 1/5] END learning_rate=1.5, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1.5, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1.5, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1.5, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1.5, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1.5, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1.5, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1.5, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1.5, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1.5, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1.5, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1.5, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1.5, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1.5, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1.5, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1.5, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1.5, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1.5, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=2, loss=linear, n_estimators=100;, score=0.795 total time=   2.5s\n",
      "[CV 2/5] END learning_rate=2, loss=linear, n_estimators=100;, score=0.753 total time=   2.4s\n",
      "[CV 3/5] END learning_rate=2, loss=linear, n_estimators=100;, score=0.781 total time=   2.5s\n",
      "[CV 4/5] END learning_rate=2, loss=linear, n_estimators=100;, score=0.777 total time=   2.5s\n",
      "[CV 5/5] END learning_rate=2, loss=linear, n_estimators=100;, score=0.805 total time=   2.4s\n",
      "[CV 1/5] END learning_rate=2, loss=linear, n_estimators=150;, score=0.804 total time=   3.5s\n",
      "[CV 2/5] END learning_rate=2, loss=linear, n_estimators=150;, score=0.773 total time=   3.5s\n",
      "[CV 3/5] END learning_rate=2, loss=linear, n_estimators=150;, score=0.799 total time=   3.5s\n",
      "[CV 4/5] END learning_rate=2, loss=linear, n_estimators=150;, score=0.794 total time=   3.5s\n",
      "[CV 5/5] END learning_rate=2, loss=linear, n_estimators=150;, score=0.816 total time=   3.5s\n",
      "[CV 1/5] END learning_rate=2, loss=linear, n_estimators=200;, score=0.807 total time=   4.6s\n",
      "[CV 2/5] END learning_rate=2, loss=linear, n_estimators=200;, score=0.774 total time=   4.6s\n",
      "[CV 3/5] END learning_rate=2, loss=linear, n_estimators=200;, score=0.793 total time=   4.6s\n",
      "[CV 4/5] END learning_rate=2, loss=linear, n_estimators=200;, score=0.791 total time=   4.5s\n",
      "[CV 5/5] END learning_rate=2, loss=linear, n_estimators=200;, score=0.810 total time=   4.5s\n",
      "[CV 1/5] END learning_rate=2, loss=linear, n_estimators=250;, score=0.802 total time=   5.6s\n",
      "[CV 2/5] END learning_rate=2, loss=linear, n_estimators=250;, score=0.778 total time=   5.6s\n",
      "[CV 3/5] END learning_rate=2, loss=linear, n_estimators=250;, score=0.799 total time=   5.6s\n",
      "[CV 4/5] END learning_rate=2, loss=linear, n_estimators=250;, score=0.790 total time=   5.5s\n",
      "[CV 5/5] END learning_rate=2, loss=linear, n_estimators=250;, score=0.819 total time=   5.6s\n",
      "[CV 1/5] END learning_rate=2, loss=linear, n_estimators=300;, score=0.813 total time=   6.6s\n",
      "[CV 2/5] END learning_rate=2, loss=linear, n_estimators=300;, score=0.781 total time=   6.7s\n",
      "[CV 3/5] END learning_rate=2, loss=linear, n_estimators=300;, score=0.800 total time=   6.7s\n",
      "[CV 4/5] END learning_rate=2, loss=linear, n_estimators=300;, score=0.793 total time=   6.6s\n",
      "[CV 5/5] END learning_rate=2, loss=linear, n_estimators=300;, score=0.821 total time=   6.6s\n",
      "[CV 1/5] END learning_rate=2, loss=square, n_estimators=100;, score=0.672 total time=   1.1s\n",
      "[CV 2/5] END learning_rate=2, loss=square, n_estimators=100;, score=0.485 total time=   1.0s\n",
      "[CV 3/5] END learning_rate=2, loss=square, n_estimators=100;, score=0.693 total time=   1.2s\n",
      "[CV 4/5] END learning_rate=2, loss=square, n_estimators=100;, score=0.595 total time=   1.1s\n",
      "[CV 5/5] END learning_rate=2, loss=square, n_estimators=100;, score=0.617 total time=   1.0s\n",
      "[CV 1/5] END learning_rate=2, loss=square, n_estimators=150;, score=0.582 total time=   1.3s\n",
      "[CV 2/5] END learning_rate=2, loss=square, n_estimators=150;, score=0.562 total time=   1.2s\n",
      "[CV 3/5] END learning_rate=2, loss=square, n_estimators=150;, score=0.399 total time=   1.2s\n",
      "[CV 4/5] END learning_rate=2, loss=square, n_estimators=150;, score=0.248 total time=   1.2s\n",
      "[CV 5/5] END learning_rate=2, loss=square, n_estimators=150;, score=0.399 total time=   1.2s\n",
      "[CV 1/5] END learning_rate=2, loss=square, n_estimators=200;, score=0.629 total time=   1.6s\n",
      "[CV 2/5] END learning_rate=2, loss=square, n_estimators=200;, score=-0.069 total time=   1.1s\n",
      "[CV 3/5] END learning_rate=2, loss=square, n_estimators=200;, score=-0.060 total time=   1.2s\n",
      "[CV 4/5] END learning_rate=2, loss=square, n_estimators=200;, score=-0.080 total time=   1.4s\n",
      "[CV 5/5] END learning_rate=2, loss=square, n_estimators=200;, score=-0.080 total time=   1.2s\n",
      "[CV 1/5] END learning_rate=2, loss=square, n_estimators=250;, score=-0.018 total time=   1.5s\n",
      "[CV 2/5] END learning_rate=2, loss=square, n_estimators=250;, score=0.299 total time=   1.9s\n",
      "[CV 3/5] END learning_rate=2, loss=square, n_estimators=250;, score=0.634 total time=   2.0s\n",
      "[CV 4/5] END learning_rate=2, loss=square, n_estimators=250;, score=-0.646 total time=   1.5s\n",
      "[CV 5/5] END learning_rate=2, loss=square, n_estimators=250;, score=-0.379 total time=   1.4s\n",
      "[CV 1/5] END learning_rate=2, loss=square, n_estimators=300;, score=-0.177 total time=   1.6s\n",
      "[CV 2/5] END learning_rate=2, loss=square, n_estimators=300;, score=0.112 total time=   2.0s\n",
      "[CV 3/5] END learning_rate=2, loss=square, n_estimators=300;, score=0.106 total time=   1.8s\n",
      "[CV 4/5] END learning_rate=2, loss=square, n_estimators=300;, score=-0.768 total time=   1.6s\n",
      "[CV 5/5] END learning_rate=2, loss=square, n_estimators=300;, score=0.169 total time=   1.8s\n",
      "[CV 1/5] END learning_rate=2, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=2, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=2, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=2, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=2, loss=exponentia, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=2, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=2, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=2, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=2, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=2, loss=exponentia, n_estimators=150;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=2, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=2, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=2, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=2, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=2, loss=exponentia, n_estimators=200;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=2, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=2, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=2, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=2, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=2, loss=exponentia, n_estimators=250;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=2, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=2, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=2, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=2, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=2, loss=exponentia, n_estimators=300;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "150 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "150 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py\", line 124, in fit\n",
      "    self._validate_params()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 570, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of AdaBoostRegressor must be a str among {'exponential', 'square', 'linear'}. Got 'exponentia' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [ 0.50329039  0.50931947  0.51152239  0.51138614  0.51299295  0.50758519\n",
      "  0.5082972   0.51158663  0.51194352  0.51564854         nan         nan\n",
      "         nan         nan         nan  0.51224196  0.52258936  0.52556122\n",
      "  0.5290009   0.53252621  0.51935064  0.52450708  0.53095242  0.53608227\n",
      "  0.53959841         nan         nan         nan         nan         nan\n",
      "  0.6227109   0.66064301  0.68271831  0.70691419  0.72049849  0.66050099\n",
      "  0.70178038  0.72915783  0.74444387  0.75559856         nan         nan\n",
      "         nan         nan         nan  0.77018887  0.7831471   0.7879779\n",
      "  0.78977379  0.79079788  0.81075373  0.82164777  0.82398536  0.82753236\n",
      "  0.83019222         nan         nan         nan         nan         nan\n",
      "  0.78242524  0.78431743  0.79024378  0.79521005  0.79627852  0.82116057\n",
      "  0.83283763  0.84383719  0.84392982  0.84804575         nan         nan\n",
      "         nan         nan         nan  0.78206456  0.79731772  0.79492803\n",
      "  0.7975082   0.80181233  0.61270045  0.43798     0.06774682 -0.02196163\n",
      " -0.1115842          nan         nan         nan         nan         nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=AdaBoostRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.001, 0.005, 0.1, 1, 1.5, 2],\n",
       "                         &#x27;loss&#x27;: [&#x27;linear&#x27;, &#x27;square&#x27;, &#x27;exponentia&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 200, 250, 300]},\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=AdaBoostRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.001, 0.005, 0.1, 1, 1.5, 2],\n",
       "                         &#x27;loss&#x27;: [&#x27;linear&#x27;, &#x27;square&#x27;, &#x27;exponentia&#x27;],\n",
       "                         &#x27;n_estimators&#x27;: [100, 150, 200, 250, 300]},\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: AdaBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>AdaBoostRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>AdaBoostRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=AdaBoostRegressor(),\n",
       "             param_grid={'learning_rate': [0.001, 0.005, 0.1, 1, 1.5, 2],\n",
       "                         'loss': ['linear', 'square', 'exponentia'],\n",
       "                         'n_estimators': [100, 150, 200, 250, 300]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train ,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec58997-c1eb-4321-be03-8236736c9bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostRegressor(learning_rate=1.5, loss=&#x27;square&#x27;, n_estimators=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostRegressor</label><div class=\"sk-toggleable__content\"><pre>AdaBoostRegressor(learning_rate=1.5, loss=&#x27;square&#x27;, n_estimators=300)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostRegressor(learning_rate=1.5, loss='square', n_estimators=300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81e90a4b-8948-4669-bfcc-95912b788bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8480457473127408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a9b4a-3eb1-40f0-800d-9f72c18dc37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21675a0-76c5-4a90-b902-6dd06cb64ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
